{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CBCYekt99VTk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.models import Sequential, Model, load_model, model_from_json\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Input, Convolution2D, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D, Flatten, Dropout, Activation, Reshape\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical  \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix, classification_report, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bKrWfcrR9V-g"
   },
   "outputs": [],
   "source": [
    "start1=time.time()\n",
    "def Inception_block(input_layer, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4): \n",
    "    # Input: \n",
    "    # - f1: number of filters of the 1x1 convolutional layer in the first path\n",
    "    # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path\n",
    "    # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path\n",
    "    # - f4: number of filters of the 1x1 convolutional layer in the fourth path\n",
    "\n",
    "    # 1st path:\n",
    "    path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
    "\n",
    "    # 2nd path\n",
    "    path2 = Conv2D(filters = f2_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
    "    path2 = Conv2D(filters = f2_conv3, kernel_size = (3,3), padding = 'same', activation = 'relu')(path2)\n",
    "\n",
    "    # 3rd path\n",
    "    path3 = Conv2D(filters = f3_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n",
    "    path3 = Conv2D(filters = f3_conv5, kernel_size = (5,5), padding = 'same', activation = 'relu')(path3)\n",
    "\n",
    "    # 4th path\n",
    "    path4 = MaxPooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)\n",
    "    path4 = Conv2D(filters = f4, kernel_size = (1,1), padding = 'same', activation = 'relu')(path4)\n",
    "\n",
    "    output_layer = concatenate([path1, path2, path3, path4], axis = -1)\n",
    "\n",
    "    return output_layer\n",
    "end1=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WeEqpmmc9bCe"
   },
   "outputs": [],
   "source": [
    "start2=time.time()\n",
    "def GoogleNet():\n",
    "    # input layer \n",
    "    img_shape=256\n",
    "    num_class=8 #v4\n",
    "    input_layer = Input(shape = (img_shape, img_shape, 1))\n",
    "\n",
    "    # convolutional layer: filters = 64, kernel_size = (7,7), strides = 2\n",
    "    X = Conv2D(filters = 64, kernel_size = (7,7), strides = 2, padding = 'valid', activation = 'relu')(input_layer)\n",
    "\n",
    "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
    "    X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n",
    "\n",
    "    # convolutional layer: filters = 64, strides = 1\n",
    "    X = Conv2D(filters = 64, kernel_size = (1,1), strides = 1, padding = 'same', activation = 'relu')(X)\n",
    "\n",
    "    # convolutional layer: filters = 192, kernel_size = (3,3)\n",
    "    X = Conv2D(filters = 192, kernel_size = (3,3), padding = 'same', activation = 'relu')(X)\n",
    "\n",
    "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
    "    X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n",
    "\n",
    "    # 1st Inception block\n",
    "    X = Inception_block(X, f1 = 64, f2_conv1 = 96, f2_conv3 = 128, f3_conv1 = 16, f3_conv5 = 32, f4 = 32)\n",
    "\n",
    "    # 2nd Inception block\n",
    "    X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 192, f3_conv1 = 32, f3_conv5 = 96, f4 = 64)\n",
    "\n",
    "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
    "    X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n",
    "\n",
    "    # 3rd Inception block\n",
    "    X = Inception_block(X, f1 = 192, f2_conv1 = 96, f2_conv3 = 208, f3_conv1 = 16, f3_conv5 = 48, f4 = 64)\n",
    "\n",
    "    # Extra network 1:\n",
    "    X1 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n",
    "    X1 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X1)\n",
    "    X1 = Flatten()(X1)\n",
    "    X1 = Dense(1024, activation = 'relu')(X1)\n",
    "    X1 = Dropout(0.7)(X1)\n",
    "    X1 = Dense(5, activation = 'softmax')(X1)\n",
    "\n",
    "\n",
    "    # 4th Inception block\n",
    "    X = Inception_block(X, f1 = 160, f2_conv1 = 112, f2_conv3 = 224, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n",
    "\n",
    "    # 5th Inception block\n",
    "    X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 256, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n",
    "\n",
    "    # 6th Inception block\n",
    "    X = Inception_block(X, f1 = 112, f2_conv1 = 144, f2_conv3 = 288, f3_conv1 = 32, f3_conv5 = 64, f4 = 64)\n",
    "\n",
    "    # Extra network 2:\n",
    "    X2 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n",
    "    X2 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X2)\n",
    "    X2 = Flatten()(X2)\n",
    "    X2 = Dense(1024, activation = 'relu')(X2)\n",
    "    X2 = Dropout(0.7)(X2)\n",
    "    X2 = Dense(1000, activation = 'softmax')(X2)\n",
    "\n",
    "\n",
    "    # 7th Inception block\n",
    "    X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, \n",
    "                      f3_conv5 = 128, f4 = 128)\n",
    "\n",
    "    # max-pooling layer: pool_size = (3,3), strides = 2\n",
    "    X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n",
    "\n",
    "    # 8th Inception block\n",
    "    X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, f3_conv5 = 128, f4 = 128)\n",
    "\n",
    "    # 9th Inception block\n",
    "    X = Inception_block(X, f1 = 384, f2_conv1 = 192, f2_conv3 = 384, f3_conv1 = 48, f3_conv5 = 128, f4 = 128)\n",
    "\n",
    "    # Global Average pooling layer \n",
    "    X = GlobalAveragePooling2D(name = 'GAPL')(X)\n",
    "    \n",
    "    X = Dense(64, activation = 'relu')(X)\n",
    "        \n",
    "    # Dropoutlayer \n",
    "    X = Dropout(0.6)(X)\n",
    "\n",
    "    X = Dense(500*num_class, activation = 'relu')(X)\n",
    "\n",
    "    X = Reshape((500, num_class))(X)\n",
    "\n",
    "    X = Activation('softmax', name=\"output\")(X)\n",
    "\n",
    "    # model\n",
    "    model = Model(inputs=input_layer, outputs=[X], name = 'GoogleNet')\n",
    "\n",
    "    return model\n",
    "end2=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uMehMDI3eVja"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "X Train: 2054\n",
      "X Test: 1074\n",
      "Y Train: 2054\n",
      "Y Test: 1074\n"
     ]
    }
   ],
   "source": [
    "#/global/scratch/users/cpezov/AAPBO/images/BenchmarkX/file_name.png\n",
    "#/global/scratch/users/cpezov/AAPBO/labels.txt\n",
    "\n",
    "x_train_all = []\n",
    "y_train_all = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "main_path=\"/global/scratch/users/cpezov/AAPBO/\"\n",
    "d_path = main_path + \"datasets-v4/\"\n",
    "i_path = main_path + \"images/\"\n",
    "\n",
    "num_class=8 #7 solvers and \"No solution\"   #v4\n",
    "\n",
    "print(\"start\")\n",
    "\n",
    "#Get each one of the images, in order\n",
    "#Randomly assign to test/train set, per family\n",
    "random.seed(9)\n",
    "with open(d_path+\"txt_files/data_ordered.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    for l in lines:\n",
    "        r = random.randint(0,2)\n",
    "        end = l.rfind(\".\")\n",
    "        img_name = i_path + l[2:end] + \".png\"\n",
    "        #print(img_name)\n",
    "        file_name = d_path + l[2:]\n",
    "        #print(file_name)\n",
    "        if(r==2):\n",
    "            x_test.append(img_to_array(load_img(img_name,color_mode=\"grayscale\")))\n",
    "        else:\n",
    "            x_train_all.append(img_to_array(load_img(img_name,color_mode=\"grayscale\")))\n",
    "        \n",
    "print(\"X Train: \"+str(len(x_train_all)))\n",
    "print(\"X Test: \"+str(len(x_test)))\n",
    "\n",
    "#read labels and transform to one-hot\n",
    "random.seed(9) #same seed to generate same random numbers as before \n",
    "with open(main_path+\"labels-v4.txt\") as f: #v4\n",
    "    lines = f.readlines()\n",
    "    for l in lines:\n",
    "        r = random.randint(0,2)\n",
    "        lbls = [int(x) for x in l.split()]\n",
    "        one_hot = to_categorical(lbls, num_classes=num_class)\n",
    "        if(r==2):\n",
    "            y_test.append(one_hot)\n",
    "        else:\n",
    "            y_train_all.append(one_hot)\n",
    "            \n",
    "print(\"Y Train: \"+str(len(y_train_all)))\n",
    "print(\"Y Test: \"+str(len(y_test)))\n",
    "            \n",
    "#x_train, x_test, y_train, y_test = model_selection.train_test_split (x, y, test_size=1./3.)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1645, 256, 256, 1)\n",
      "(1645, 500, 8)\n",
      "(409, 256, 256, 1)\n",
      "(409, 500, 8)\n"
     ]
    }
   ],
   "source": [
    "#Once separated train/test, the train set must be separated train/validation in 80:20 (4:1) ratio\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "random.seed(3)\n",
    "for i in range(len(y_train_all)):\n",
    "    r = random.randint(0,4)\n",
    "    if(r==4):\n",
    "        x_val.append(x_train_all[i])\n",
    "        y_val.append(y_train_all[i])\n",
    "    else:\n",
    "        x_train.append(x_train_all[i])\n",
    "        y_train.append(y_train_all[i])\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GoogleNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 125, 125, 64) 3200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 62, 62, 64)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 62, 62, 64)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 62, 62, 192)  110784      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 30, 30, 192)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 30, 30, 96)   18528       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 30, 30, 16)   3088        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 30, 30, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 30, 30, 64)   12352       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 30, 30, 128)  110720      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 30, 30, 32)   12832       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 30, 30, 32)   6176        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 30, 30, 256)  0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 30, 30, 128)  32896       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 30, 30, 32)   8224        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 30, 30, 256)  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 30, 30, 128)  32896       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 30, 30, 192)  221376      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 30, 30, 96)   76896       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 30, 30, 64)   16448       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 30, 480)  0           conv2d_9[0][0]                   \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 480)  0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 14, 14, 96)   46176       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 14, 14, 16)   7696        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 14, 14, 480)  0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 14, 14, 192)  92352       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 14, 14, 208)  179920      conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 14, 14, 48)   19248       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 14, 14, 64)   30784       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 14, 14, 512)  0           conv2d_15[0][0]                  \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 14, 14, 112)  57456       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 14, 14, 24)   12312       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 14, 14, 512)  0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 14, 14, 160)  82080       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 14, 14, 224)  226016      conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 14, 14, 64)   38464       conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 14, 14, 64)   32832       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 14, 14, 512)  0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 14, 14, 128)  65664       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 14, 14, 24)   12312       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 14, 14, 512)  0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 14, 14, 128)  65664       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 14, 14, 256)  295168      conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 14, 14, 64)   38464       conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 14, 14, 64)   32832       max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 14, 14, 512)  0           conv2d_28[0][0]                  \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 14, 14, 144)  73872       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 14, 14, 32)   16416       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 14, 14, 512)  0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 14, 14, 112)  57456       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 14, 14, 288)  373536      conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 14, 14, 64)   51264       conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 14, 14, 64)   32832       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 14, 14, 528)  0           conv2d_34[0][0]                  \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 14, 14, 160)  84640       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 14, 14, 32)   16928       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 14, 14, 528)  0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 14, 14, 256)  135424      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 14, 14, 320)  461120      conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 14, 14, 128)  102528      conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 14, 14, 128)  67712       max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 14, 14, 832)  0           conv2d_41[0][0]                  \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 6, 6, 832)    0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 6, 6, 160)    133280      max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 6, 6, 32)     26656       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 6, 6, 832)    0           max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 6, 6, 256)    213248      max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 6, 6, 320)    461120      conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 6, 6, 128)    102528      conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 6, 6, 128)    106624      max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 6, 6, 832)    0           conv2d_47[0][0]                  \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "                                                                 conv2d_51[0][0]                  \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 6, 6, 192)    159936      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 6, 6, 48)     39984       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 6, 6, 832)    0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 6, 6, 384)    319872      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 6, 6, 384)    663936      conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 6, 6, 128)    153728      conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 6, 6, 128)    106624      max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 6, 6, 1024)   0           conv2d_53[0][0]                  \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "                                                                 conv2d_57[0][0]                  \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "GAPL (GlobalAveragePooling2D)   (None, 1024)         0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           65600       GAPL[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4000)         260000      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 500, 8)       0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Activation)             (None, 500, 8)       0           reshape[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,292,880\n",
      "Trainable params: 6,292,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.0801 - acc: 0.1262 - mse: 0.1094WARNING:tensorflow:From /global/software/sl-7.x86_64/modules/apps/ml/tensorflow/2.3.0-py37/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/13 [===>..........................] - ETA: 6s - loss: 2.0794 - acc: 0.1296 - mse: 0.1094WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3951s vs `on_train_batch_end` time: 0.6966s). Check your callbacks.\n",
      "13/13 [==============================] - ETA: 0s - loss: 2.0756 - acc: 0.1444 - mse: 0.1092\n",
      "Epoch 00001: val_acc improved from -inf to 0.18503, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 22s 2s/step - loss: 2.0756 - acc: 0.1444 - mse: 0.1092 - val_loss: 2.0654 - val_acc: 0.1850 - val_mse: 0.1089\n",
      "Epoch 2/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 2.0567 - acc: 0.1772 - mse: 0.1086\n",
      "Epoch 00002: val_acc improved from 0.18503 to 0.24537, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 2.0567 - acc: 0.1772 - mse: 0.1086 - val_loss: 2.0121 - val_acc: 0.2454 - val_mse: 0.1069\n",
      "Epoch 3/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 2.0226 - acc: 0.2166 - mse: 0.1073\n",
      "Epoch 00003: val_acc improved from 0.24537 to 0.31482, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 2.0226 - acc: 0.2166 - mse: 0.1073 - val_loss: 1.9691 - val_acc: 0.3148 - val_mse: 0.1052\n",
      "Epoch 4/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.9734 - acc: 0.2627 - mse: 0.1053\n",
      "Epoch 00004: val_acc improved from 0.31482 to 0.38006, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 1.9734 - acc: 0.2627 - mse: 0.1053 - val_loss: 1.9153 - val_acc: 0.3801 - val_mse: 0.1028\n",
      "Epoch 5/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.9220 - acc: 0.3092 - mse: 0.1030\n",
      "Epoch 00005: val_acc improved from 0.38006 to 0.42761, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 1.9220 - acc: 0.3092 - mse: 0.1030 - val_loss: 1.8213 - val_acc: 0.4276 - val_mse: 0.0981\n",
      "Epoch 6/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.8694 - acc: 0.3535 - mse: 0.1005\n",
      "Epoch 00006: val_acc improved from 0.42761 to 0.45102, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.8694 - acc: 0.3535 - mse: 0.1005 - val_loss: 1.7923 - val_acc: 0.4510 - val_mse: 0.0966\n",
      "Epoch 7/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.8418 - acc: 0.3824 - mse: 0.0993\n",
      "Epoch 00007: val_acc improved from 0.45102 to 0.45936, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.8418 - acc: 0.3824 - mse: 0.0993 - val_loss: 1.7626 - val_acc: 0.4594 - val_mse: 0.0950\n",
      "Epoch 8/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.8124 - acc: 0.4076 - mse: 0.0979\n",
      "Epoch 00008: val_acc improved from 0.45936 to 0.46115, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 1.8124 - acc: 0.4076 - mse: 0.0979 - val_loss: 1.7614 - val_acc: 0.4611 - val_mse: 0.0950\n",
      "Epoch 9/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7994 - acc: 0.4230 - mse: 0.0972\n",
      "Epoch 00009: val_acc improved from 0.46115 to 0.46153, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7994 - acc: 0.4230 - mse: 0.0972 - val_loss: 1.7105 - val_acc: 0.4615 - val_mse: 0.0920\n",
      "Epoch 10/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7845 - acc: 0.4304 - mse: 0.0966\n",
      "Epoch 00010: val_acc improved from 0.46153 to 0.46227, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7845 - acc: 0.4304 - mse: 0.0966 - val_loss: 1.7117 - val_acc: 0.4623 - val_mse: 0.0921\n",
      "Epoch 11/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7738 - acc: 0.4380 - mse: 0.0961\n",
      "Epoch 00011: val_acc did not improve from 0.46227\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7738 - acc: 0.4380 - mse: 0.0961 - val_loss: 1.7459 - val_acc: 0.4623 - val_mse: 0.0943\n",
      "Epoch 12/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7640 - acc: 0.4419 - mse: 0.0956\n",
      "Epoch 00012: val_acc improved from 0.46227 to 0.46318, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 1.7640 - acc: 0.4419 - mse: 0.0956 - val_loss: 1.7051 - val_acc: 0.4632 - val_mse: 0.0918\n",
      "Epoch 13/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7482 - acc: 0.4475 - mse: 0.0948\n",
      "Epoch 00013: val_acc improved from 0.46318 to 0.46386, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 1.7482 - acc: 0.4475 - mse: 0.0948 - val_loss: 1.6900 - val_acc: 0.4639 - val_mse: 0.0909\n",
      "Epoch 14/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7500 - acc: 0.4483 - mse: 0.0950\n",
      "Epoch 00014: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7500 - acc: 0.4483 - mse: 0.0950 - val_loss: 1.7197 - val_acc: 0.4639 - val_mse: 0.0928\n",
      "Epoch 15/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7368 - acc: 0.4511 - mse: 0.0943\n",
      "Epoch 00015: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7368 - acc: 0.4511 - mse: 0.0943 - val_loss: 1.6821 - val_acc: 0.4639 - val_mse: 0.0904\n",
      "Epoch 16/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7305 - acc: 0.4538 - mse: 0.0940\n",
      "Epoch 00016: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 1.7305 - acc: 0.4538 - mse: 0.0940 - val_loss: 1.6841 - val_acc: 0.4639 - val_mse: 0.0906\n",
      "Epoch 17/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7252 - acc: 0.4545 - mse: 0.0938\n",
      "Epoch 00017: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7252 - acc: 0.4545 - mse: 0.0938 - val_loss: 1.7033 - val_acc: 0.4639 - val_mse: 0.0918\n",
      "Epoch 18/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7110 - acc: 0.4575 - mse: 0.0930\n",
      "Epoch 00018: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7110 - acc: 0.4575 - mse: 0.0930 - val_loss: 1.6835 - val_acc: 0.4639 - val_mse: 0.0906\n",
      "Epoch 19/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7104 - acc: 0.4576 - mse: 0.0932\n",
      "Epoch 00019: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7104 - acc: 0.4576 - mse: 0.0932 - val_loss: 1.6778 - val_acc: 0.4639 - val_mse: 0.0903\n",
      "Epoch 20/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7128 - acc: 0.4572 - mse: 0.0933\n",
      "Epoch 00020: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 1.7128 - acc: 0.4572 - mse: 0.0933 - val_loss: 1.6714 - val_acc: 0.4639 - val_mse: 0.0900\n",
      "Epoch 21/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7058 - acc: 0.4587 - mse: 0.0928\n",
      "Epoch 00021: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7058 - acc: 0.4587 - mse: 0.0928 - val_loss: 1.6762 - val_acc: 0.4639 - val_mse: 0.0903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6962 - acc: 0.4588 - mse: 0.0925\n",
      "Epoch 00022: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6962 - acc: 0.4588 - mse: 0.0925 - val_loss: 1.6781 - val_acc: 0.4639 - val_mse: 0.0905\n",
      "Epoch 23/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6930 - acc: 0.4601 - mse: 0.0922\n",
      "Epoch 00023: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6930 - acc: 0.4601 - mse: 0.0922 - val_loss: 1.7240 - val_acc: 0.4639 - val_mse: 0.0929\n",
      "Epoch 24/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7053 - acc: 0.4611 - mse: 0.0930\n",
      "Epoch 00024: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7053 - acc: 0.4611 - mse: 0.0930 - val_loss: 1.6670 - val_acc: 0.4639 - val_mse: 0.0899\n",
      "Epoch 25/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6824 - acc: 0.4627 - mse: 0.0918\n",
      "Epoch 00025: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 996ms/step - loss: 1.6824 - acc: 0.4627 - mse: 0.0918 - val_loss: 1.6875 - val_acc: 0.4639 - val_mse: 0.0911\n",
      "Epoch 26/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7027 - acc: 0.4622 - mse: 0.0928\n",
      "Epoch 00026: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 1.7027 - acc: 0.4622 - mse: 0.0928 - val_loss: 1.6821 - val_acc: 0.4639 - val_mse: 0.0910\n",
      "Epoch 27/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.7007 - acc: 0.4618 - mse: 0.0930\n",
      "Epoch 00027: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.7007 - acc: 0.4618 - mse: 0.0930 - val_loss: 1.6727 - val_acc: 0.4639 - val_mse: 0.0905\n",
      "Epoch 28/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6940 - acc: 0.4635 - mse: 0.0927\n",
      "Epoch 00028: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6940 - acc: 0.4635 - mse: 0.0927 - val_loss: 1.6668 - val_acc: 0.4639 - val_mse: 0.0901\n",
      "Epoch 29/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6774 - acc: 0.4648 - mse: 0.0918\n",
      "Epoch 00029: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 996ms/step - loss: 1.6774 - acc: 0.4648 - mse: 0.0918 - val_loss: 1.6591 - val_acc: 0.4639 - val_mse: 0.0895\n",
      "Epoch 30/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6780 - acc: 0.4640 - mse: 0.0916\n",
      "Epoch 00030: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 1.6780 - acc: 0.4640 - mse: 0.0916 - val_loss: 1.6909 - val_acc: 0.4639 - val_mse: 0.0915\n",
      "Epoch 31/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6664 - acc: 0.4650 - mse: 0.0912\n",
      "Epoch 00031: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1000ms/step - loss: 1.6664 - acc: 0.4650 - mse: 0.0912 - val_loss: 1.6543 - val_acc: 0.4639 - val_mse: 0.0891\n",
      "Epoch 32/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6650 - acc: 0.4659 - mse: 0.0909\n",
      "Epoch 00032: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6650 - acc: 0.4659 - mse: 0.0909 - val_loss: 1.6502 - val_acc: 0.4639 - val_mse: 0.0888\n",
      "Epoch 33/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6710 - acc: 0.4667 - mse: 0.0914\n",
      "Epoch 00033: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6710 - acc: 0.4667 - mse: 0.0914 - val_loss: 1.7160 - val_acc: 0.4639 - val_mse: 0.0932\n",
      "Epoch 34/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6744 - acc: 0.4669 - mse: 0.0916\n",
      "Epoch 00034: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6744 - acc: 0.4669 - mse: 0.0916 - val_loss: 1.6583 - val_acc: 0.4639 - val_mse: 0.0894\n",
      "Epoch 35/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6416 - acc: 0.4675 - mse: 0.0898\n",
      "Epoch 00035: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6416 - acc: 0.4675 - mse: 0.0898 - val_loss: 1.6655 - val_acc: 0.4639 - val_mse: 0.0896\n",
      "Epoch 36/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6479 - acc: 0.4686 - mse: 0.0900\n",
      "Epoch 00036: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6479 - acc: 0.4686 - mse: 0.0900 - val_loss: 1.6588 - val_acc: 0.4639 - val_mse: 0.0895\n",
      "Epoch 37/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6456 - acc: 0.4684 - mse: 0.0899\n",
      "Epoch 00037: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6456 - acc: 0.4684 - mse: 0.0899 - val_loss: 1.6957 - val_acc: 0.4639 - val_mse: 0.0916\n",
      "Epoch 38/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6569 - acc: 0.4692 - mse: 0.0904\n",
      "Epoch 00038: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6569 - acc: 0.4692 - mse: 0.0904 - val_loss: 1.6519 - val_acc: 0.4639 - val_mse: 0.0890\n",
      "Epoch 39/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6516 - acc: 0.4692 - mse: 0.0900\n",
      "Epoch 00039: val_acc did not improve from 0.46386\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 1.6516 - acc: 0.4692 - mse: 0.0900 - val_loss: 1.6467 - val_acc: 0.4639 - val_mse: 0.0882\n",
      "Epoch 40/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6298 - acc: 0.4699 - mse: 0.0887\n",
      "Epoch 00040: val_acc improved from 0.46386 to 0.46691, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 1.6298 - acc: 0.4699 - mse: 0.0887 - val_loss: 1.6696 - val_acc: 0.4669 - val_mse: 0.0893\n",
      "Epoch 41/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6103 - acc: 0.4712 - mse: 0.0876\n",
      "Epoch 00041: val_acc did not improve from 0.46691\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6103 - acc: 0.4712 - mse: 0.0876 - val_loss: 1.6572 - val_acc: 0.4640 - val_mse: 0.0891\n",
      "Epoch 42/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6158 - acc: 0.4724 - mse: 0.0878\n",
      "Epoch 00042: val_acc did not improve from 0.46691\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6158 - acc: 0.4724 - mse: 0.0878 - val_loss: 1.6372 - val_acc: 0.4639 - val_mse: 0.0878\n",
      "Epoch 43/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6087 - acc: 0.4728 - mse: 0.0875\n",
      "Epoch 00043: val_acc did not improve from 0.46691\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6087 - acc: 0.4728 - mse: 0.0875 - val_loss: 1.6643 - val_acc: 0.4639 - val_mse: 0.0889\n",
      "Epoch 44/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6145 - acc: 0.4727 - mse: 0.0876\n",
      "Epoch 00044: val_acc did not improve from 0.46691\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6145 - acc: 0.4727 - mse: 0.0876 - val_loss: 1.6536 - val_acc: 0.4648 - val_mse: 0.0878\n",
      "Epoch 45/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6004 - acc: 0.4751 - mse: 0.0868\n",
      "Epoch 00045: val_acc did not improve from 0.46691\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6004 - acc: 0.4751 - mse: 0.0868 - val_loss: 1.6378 - val_acc: 0.4639 - val_mse: 0.0876\n",
      "Epoch 46/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5940 - acc: 0.4732 - mse: 0.0864\n",
      "Epoch 00046: val_acc did not improve from 0.46691\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.5940 - acc: 0.4732 - mse: 0.0864 - val_loss: 1.6342 - val_acc: 0.4645 - val_mse: 0.0870\n",
      "Epoch 47/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5752 - acc: 0.4768 - mse: 0.0852\n",
      "Epoch 00047: val_acc improved from 0.46691 to 0.46840, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.5752 - acc: 0.4768 - mse: 0.0852 - val_loss: 1.6473 - val_acc: 0.4684 - val_mse: 0.0874\n",
      "Epoch 48/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - ETA: 0s - loss: 1.5876 - acc: 0.4773 - mse: 0.0862\n",
      "Epoch 00048: val_acc improved from 0.46840 to 0.47182, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 1.5876 - acc: 0.4773 - mse: 0.0862 - val_loss: 1.6449 - val_acc: 0.4718 - val_mse: 0.0885\n",
      "Epoch 49/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6043 - acc: 0.4779 - mse: 0.0874\n",
      "Epoch 00049: val_acc did not improve from 0.47182\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6043 - acc: 0.4779 - mse: 0.0874 - val_loss: 1.6265 - val_acc: 0.4647 - val_mse: 0.0870\n",
      "Epoch 50/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6180 - acc: 0.4779 - mse: 0.0882\n",
      "Epoch 00050: val_acc did not improve from 0.47182\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.6180 - acc: 0.4779 - mse: 0.0882 - val_loss: 1.6293 - val_acc: 0.4669 - val_mse: 0.0875\n",
      "Epoch 51/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5887 - acc: 0.4767 - mse: 0.0866\n",
      "Epoch 00051: val_acc improved from 0.47182 to 0.47497, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 1.5887 - acc: 0.4767 - mse: 0.0866 - val_loss: 1.6407 - val_acc: 0.4750 - val_mse: 0.0883\n",
      "Epoch 52/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5396 - acc: 0.4862 - mse: 0.0836\n",
      "Epoch 00052: val_acc did not improve from 0.47497\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.5396 - acc: 0.4862 - mse: 0.0836 - val_loss: 1.6709 - val_acc: 0.4673 - val_mse: 0.0890\n",
      "Epoch 53/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5333 - acc: 0.4730 - mse: 0.0830\n",
      "Epoch 00053: val_acc did not improve from 0.47497\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.5333 - acc: 0.4730 - mse: 0.0830 - val_loss: 1.6627 - val_acc: 0.4671 - val_mse: 0.0879\n",
      "Epoch 54/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5103 - acc: 0.4842 - mse: 0.0816\n",
      "Epoch 00054: val_acc did not improve from 0.47497\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.5103 - acc: 0.4842 - mse: 0.0816 - val_loss: 1.8355 - val_acc: 0.4644 - val_mse: 0.0933\n",
      "Epoch 55/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5000 - acc: 0.4911 - mse: 0.0810\n",
      "Epoch 00055: val_acc did not improve from 0.47497\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.5000 - acc: 0.4911 - mse: 0.0810 - val_loss: 1.7322 - val_acc: 0.4705 - val_mse: 0.0926\n",
      "Epoch 56/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5000 - acc: 0.4914 - mse: 0.0811\n",
      "Epoch 00056: val_acc did not improve from 0.47497\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.5000 - acc: 0.4914 - mse: 0.0811 - val_loss: 1.7593 - val_acc: 0.4629 - val_mse: 0.0909\n",
      "Epoch 57/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.4659 - acc: 0.5008 - mse: 0.0792\n",
      "Epoch 00057: val_acc did not improve from 0.47497\n",
      "13/13 [==============================] - 13s 996ms/step - loss: 1.4659 - acc: 0.5008 - mse: 0.0792 - val_loss: 1.7090 - val_acc: 0.4710 - val_mse: 0.0894\n",
      "Epoch 58/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.4866 - acc: 0.5022 - mse: 0.0805\n",
      "Epoch 00058: val_acc did not improve from 0.47497\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 1.4866 - acc: 0.5022 - mse: 0.0805 - val_loss: 1.7085 - val_acc: 0.4343 - val_mse: 0.0908\n",
      "Epoch 59/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.4136 - acc: 0.5139 - mse: 0.0765\n",
      "Epoch 00059: val_acc improved from 0.47497 to 0.47752, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.4136 - acc: 0.5139 - mse: 0.0765 - val_loss: 1.7034 - val_acc: 0.4775 - val_mse: 0.0875\n",
      "Epoch 60/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3765 - acc: 0.5270 - mse: 0.0743\n",
      "Epoch 00060: val_acc did not improve from 0.47752\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 1.3765 - acc: 0.5270 - mse: 0.0743 - val_loss: 1.7252 - val_acc: 0.4464 - val_mse: 0.0888\n",
      "Epoch 61/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3458 - acc: 0.5371 - mse: 0.0725\n",
      "Epoch 00061: val_acc did not improve from 0.47752\n",
      "13/13 [==============================] - 13s 994ms/step - loss: 1.3458 - acc: 0.5371 - mse: 0.0725 - val_loss: 1.7435 - val_acc: 0.4702 - val_mse: 0.0885\n",
      "Epoch 62/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2955 - acc: 0.5441 - mse: 0.0699\n",
      "Epoch 00062: val_acc did not improve from 0.47752\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.2955 - acc: 0.5441 - mse: 0.0699 - val_loss: 1.7125 - val_acc: 0.4448 - val_mse: 0.0910\n",
      "Epoch 63/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3175 - acc: 0.5480 - mse: 0.0710\n",
      "Epoch 00063: val_acc did not improve from 0.47752\n",
      "13/13 [==============================] - 13s 1000ms/step - loss: 1.3175 - acc: 0.5480 - mse: 0.0710 - val_loss: 1.8926 - val_acc: 0.4766 - val_mse: 0.0918\n",
      "Epoch 64/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2537 - acc: 0.5579 - mse: 0.0680\n",
      "Epoch 00064: val_acc did not improve from 0.47752\n",
      "13/13 [==============================] - 13s 996ms/step - loss: 1.2537 - acc: 0.5579 - mse: 0.0680 - val_loss: 1.8647 - val_acc: 0.4681 - val_mse: 0.0902\n",
      "Epoch 65/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2354 - acc: 0.5653 - mse: 0.0670\n",
      "Epoch 00065: val_acc did not improve from 0.47752\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 1.2354 - acc: 0.5653 - mse: 0.0670 - val_loss: 1.8195 - val_acc: 0.4712 - val_mse: 0.0901\n",
      "Epoch 66/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2158 - acc: 0.5667 - mse: 0.0660\n",
      "Epoch 00066: val_acc did not improve from 0.47752\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 1.2158 - acc: 0.5667 - mse: 0.0660 - val_loss: 1.9507 - val_acc: 0.4749 - val_mse: 0.0912\n",
      "Epoch 67/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2287 - acc: 0.5694 - mse: 0.0666\n",
      "Epoch 00067: val_acc improved from 0.47752 to 0.48143, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.2287 - acc: 0.5694 - mse: 0.0666 - val_loss: 1.9841 - val_acc: 0.4814 - val_mse: 0.0921\n",
      "Epoch 68/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1833 - acc: 0.5735 - mse: 0.0649\n",
      "Epoch 00068: val_acc did not improve from 0.48143\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 1.1833 - acc: 0.5735 - mse: 0.0649 - val_loss: 1.9395 - val_acc: 0.4710 - val_mse: 0.0916\n",
      "Epoch 69/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1938 - acc: 0.5738 - mse: 0.0650\n",
      "Epoch 00069: val_acc did not improve from 0.48143\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 1.1938 - acc: 0.5738 - mse: 0.0650 - val_loss: 1.8691 - val_acc: 0.4588 - val_mse: 0.0908\n",
      "Epoch 70/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1791 - acc: 0.5763 - mse: 0.0650\n",
      "Epoch 00070: val_acc did not improve from 0.48143\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 1.1791 - acc: 0.5763 - mse: 0.0650 - val_loss: 1.9183 - val_acc: 0.4702 - val_mse: 0.0901\n",
      "Epoch 71/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1469 - acc: 0.5831 - mse: 0.0633\n",
      "Epoch 00071: val_acc did not improve from 0.48143\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 1.1469 - acc: 0.5831 - mse: 0.0633 - val_loss: 1.9127 - val_acc: 0.4745 - val_mse: 0.0890\n",
      "Epoch 72/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1522 - acc: 0.5790 - mse: 0.0636\n",
      "Epoch 00072: val_acc did not improve from 0.48143\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 1.1522 - acc: 0.5790 - mse: 0.0636 - val_loss: 1.7876 - val_acc: 0.4142 - val_mse: 0.0921\n",
      "Epoch 73/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2232 - acc: 0.5647 - mse: 0.0672\n",
      "Epoch 00073: val_acc did not improve from 0.48143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 13s 999ms/step - loss: 1.2232 - acc: 0.5647 - mse: 0.0672 - val_loss: 1.7850 - val_acc: 0.4583 - val_mse: 0.0892\n",
      "Epoch 74/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1474 - acc: 0.5810 - mse: 0.0635\n",
      "Epoch 00074: val_acc did not improve from 0.48143\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.1474 - acc: 0.5810 - mse: 0.0635 - val_loss: 1.9606 - val_acc: 0.4646 - val_mse: 0.0908\n",
      "Epoch 75/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0996 - acc: 0.5879 - mse: 0.0613\n",
      "Epoch 00075: val_acc improved from 0.48143 to 0.48516, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.0996 - acc: 0.5879 - mse: 0.0613 - val_loss: 2.0748 - val_acc: 0.4852 - val_mse: 0.0904\n",
      "Epoch 76/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0982 - acc: 0.5854 - mse: 0.0615\n",
      "Epoch 00076: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.0982 - acc: 0.5854 - mse: 0.0615 - val_loss: 1.8937 - val_acc: 0.4576 - val_mse: 0.0887\n",
      "Epoch 77/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0744 - acc: 0.5941 - mse: 0.0598\n",
      "Epoch 00077: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.0744 - acc: 0.5941 - mse: 0.0598 - val_loss: 2.0632 - val_acc: 0.4721 - val_mse: 0.0896\n",
      "Epoch 78/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0607 - acc: 0.5976 - mse: 0.0596\n",
      "Epoch 00078: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.0607 - acc: 0.5976 - mse: 0.0596 - val_loss: 2.0999 - val_acc: 0.4741 - val_mse: 0.0898\n",
      "Epoch 79/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0336 - acc: 0.6057 - mse: 0.0586\n",
      "Epoch 00079: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 1.0336 - acc: 0.6057 - mse: 0.0586 - val_loss: 2.0595 - val_acc: 0.4777 - val_mse: 0.0886\n",
      "Epoch 80/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0271 - acc: 0.6096 - mse: 0.0577\n",
      "Epoch 00080: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.0271 - acc: 0.6096 - mse: 0.0577 - val_loss: 2.0658 - val_acc: 0.4560 - val_mse: 0.0898\n",
      "Epoch 81/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0051 - acc: 0.6153 - mse: 0.0568\n",
      "Epoch 00081: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.0051 - acc: 0.6153 - mse: 0.0568 - val_loss: 2.2332 - val_acc: 0.4778 - val_mse: 0.0894\n",
      "Epoch 82/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9995 - acc: 0.6203 - mse: 0.0565\n",
      "Epoch 00082: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.9995 - acc: 0.6203 - mse: 0.0565 - val_loss: 2.6339 - val_acc: 0.4750 - val_mse: 0.0929\n",
      "Epoch 83/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.0195 - acc: 0.6218 - mse: 0.0567\n",
      "Epoch 00083: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.0195 - acc: 0.6218 - mse: 0.0567 - val_loss: 2.4581 - val_acc: 0.4629 - val_mse: 0.0936\n",
      "Epoch 84/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9909 - acc: 0.6293 - mse: 0.0561\n",
      "Epoch 00084: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.9909 - acc: 0.6293 - mse: 0.0561 - val_loss: 1.9575 - val_acc: 0.4663 - val_mse: 0.0879\n",
      "Epoch 85/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9586 - acc: 0.6390 - mse: 0.0542\n",
      "Epoch 00085: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.9586 - acc: 0.6390 - mse: 0.0542 - val_loss: 2.2341 - val_acc: 0.4408 - val_mse: 0.0912\n",
      "Epoch 86/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9680 - acc: 0.6204 - mse: 0.0554\n",
      "Epoch 00086: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.9680 - acc: 0.6204 - mse: 0.0554 - val_loss: 2.1328 - val_acc: 0.4659 - val_mse: 0.0885\n",
      "Epoch 87/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9307 - acc: 0.6449 - mse: 0.0531\n",
      "Epoch 00087: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 994ms/step - loss: 0.9307 - acc: 0.6449 - mse: 0.0531 - val_loss: 2.3731 - val_acc: 0.4846 - val_mse: 0.0895\n",
      "Epoch 88/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9300 - acc: 0.6547 - mse: 0.0525\n",
      "Epoch 00088: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.9300 - acc: 0.6547 - mse: 0.0525 - val_loss: 2.4970 - val_acc: 0.4805 - val_mse: 0.0905\n",
      "Epoch 89/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9426 - acc: 0.6474 - mse: 0.0536\n",
      "Epoch 00089: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 996ms/step - loss: 0.9426 - acc: 0.6474 - mse: 0.0536 - val_loss: 2.3641 - val_acc: 0.4758 - val_mse: 0.0915\n",
      "Epoch 90/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9201 - acc: 0.6519 - mse: 0.0527\n",
      "Epoch 00090: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 0.9201 - acc: 0.6519 - mse: 0.0527 - val_loss: 2.4867 - val_acc: 0.4644 - val_mse: 0.0916\n",
      "Epoch 91/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9153 - acc: 0.6629 - mse: 0.0525\n",
      "Epoch 00091: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.9153 - acc: 0.6629 - mse: 0.0525 - val_loss: 2.0852 - val_acc: 0.4394 - val_mse: 0.0894\n",
      "Epoch 92/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9015 - acc: 0.6607 - mse: 0.0517\n",
      "Epoch 00092: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 1000ms/step - loss: 0.9015 - acc: 0.6607 - mse: 0.0517 - val_loss: 2.1130 - val_acc: 0.4602 - val_mse: 0.0891\n",
      "Epoch 93/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8851 - acc: 0.6713 - mse: 0.0509\n",
      "Epoch 00093: val_acc did not improve from 0.48516\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.8851 - acc: 0.6713 - mse: 0.0509 - val_loss: 2.2025 - val_acc: 0.4630 - val_mse: 0.0892\n",
      "Epoch 94/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8753 - acc: 0.6755 - mse: 0.0503\n",
      "Epoch 00094: val_acc improved from 0.48516 to 0.48664, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.8753 - acc: 0.6755 - mse: 0.0503 - val_loss: 2.4453 - val_acc: 0.4866 - val_mse: 0.0900\n",
      "Epoch 95/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9020 - acc: 0.6696 - mse: 0.0516\n",
      "Epoch 00095: val_acc did not improve from 0.48664\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 0.9020 - acc: 0.6696 - mse: 0.0516 - val_loss: 2.0933 - val_acc: 0.4450 - val_mse: 0.0894\n",
      "Epoch 96/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8758 - acc: 0.6764 - mse: 0.0503\n",
      "Epoch 00096: val_acc did not improve from 0.48664\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 0.8758 - acc: 0.6764 - mse: 0.0503 - val_loss: 2.2051 - val_acc: 0.4716 - val_mse: 0.0888\n",
      "Epoch 97/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8326 - acc: 0.6973 - mse: 0.0482\n",
      "Epoch 00097: val_acc did not improve from 0.48664\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 0.8326 - acc: 0.6973 - mse: 0.0482 - val_loss: 2.2043 - val_acc: 0.4312 - val_mse: 0.0911\n",
      "Epoch 98/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8123 - acc: 0.7153 - mse: 0.0469\n",
      "Epoch 00098: val_acc did not improve from 0.48664\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 0.8123 - acc: 0.7153 - mse: 0.0469 - val_loss: 2.2564 - val_acc: 0.4678 - val_mse: 0.0896\n",
      "Epoch 99/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8270 - acc: 0.7035 - mse: 0.0483\n",
      "Epoch 00099: val_acc did not improve from 0.48664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 13s 999ms/step - loss: 0.8270 - acc: 0.7035 - mse: 0.0483 - val_loss: 1.9246 - val_acc: 0.3962 - val_mse: 0.0913\n",
      "Epoch 100/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8968 - acc: 0.6840 - mse: 0.0515\n",
      "Epoch 00100: val_acc did not improve from 0.48664\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 0.8968 - acc: 0.6840 - mse: 0.0515 - val_loss: 2.1281 - val_acc: 0.4595 - val_mse: 0.0894\n",
      "Epoch 101/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8357 - acc: 0.6990 - mse: 0.0486\n",
      "Epoch 00101: val_acc did not improve from 0.48664\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.8357 - acc: 0.6990 - mse: 0.0486 - val_loss: 2.4269 - val_acc: 0.4791 - val_mse: 0.0915\n",
      "Epoch 102/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8281 - acc: 0.7043 - mse: 0.0482\n",
      "Epoch 00102: val_acc did not improve from 0.48664\n",
      "13/13 [==============================] - 13s 994ms/step - loss: 0.8281 - acc: 0.7043 - mse: 0.0482 - val_loss: 2.2743 - val_acc: 0.4709 - val_mse: 0.0896\n",
      "Epoch 103/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8173 - acc: 0.7134 - mse: 0.0472\n",
      "Epoch 00103: val_acc did not improve from 0.48664\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 0.8173 - acc: 0.7134 - mse: 0.0472 - val_loss: 2.1536 - val_acc: 0.4544 - val_mse: 0.0886\n",
      "Epoch 104/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7881 - acc: 0.7195 - mse: 0.0460\n",
      "Epoch 00104: val_acc did not improve from 0.48664\n",
      "13/13 [==============================] - 13s 994ms/step - loss: 0.7881 - acc: 0.7195 - mse: 0.0460 - val_loss: 2.3352 - val_acc: 0.4713 - val_mse: 0.0900\n",
      "Epoch 105/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7767 - acc: 0.7258 - mse: 0.0453\n",
      "Epoch 00105: val_acc did not improve from 0.48664\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7767 - acc: 0.7258 - mse: 0.0453 - val_loss: 2.3320 - val_acc: 0.4667 - val_mse: 0.0896\n",
      "Epoch 106/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7635 - acc: 0.7302 - mse: 0.0445\n",
      "Epoch 00106: val_acc improved from 0.48664 to 0.48767, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7635 - acc: 0.7302 - mse: 0.0445 - val_loss: 2.3694 - val_acc: 0.4877 - val_mse: 0.0880\n",
      "Epoch 107/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7706 - acc: 0.7326 - mse: 0.0447\n",
      "Epoch 00107: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 0.7706 - acc: 0.7326 - mse: 0.0447 - val_loss: 2.1339 - val_acc: 0.4283 - val_mse: 0.0910\n",
      "Epoch 108/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7636 - acc: 0.7307 - mse: 0.0448\n",
      "Epoch 00108: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.7636 - acc: 0.7307 - mse: 0.0448 - val_loss: 2.1178 - val_acc: 0.4425 - val_mse: 0.0891\n",
      "Epoch 109/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7523 - acc: 0.7318 - mse: 0.0442\n",
      "Epoch 00109: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 0.7523 - acc: 0.7318 - mse: 0.0442 - val_loss: 2.0613 - val_acc: 0.4244 - val_mse: 0.0895\n",
      "Epoch 110/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7537 - acc: 0.7358 - mse: 0.0441\n",
      "Epoch 00110: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 995ms/step - loss: 0.7537 - acc: 0.7358 - mse: 0.0441 - val_loss: 2.4372 - val_acc: 0.4415 - val_mse: 0.0933\n",
      "Epoch 111/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7767 - acc: 0.7257 - mse: 0.0455\n",
      "Epoch 00111: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.7767 - acc: 0.7257 - mse: 0.0455 - val_loss: 2.2533 - val_acc: 0.4771 - val_mse: 0.0891\n",
      "Epoch 112/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7948 - acc: 0.7269 - mse: 0.0462\n",
      "Epoch 00112: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7948 - acc: 0.7269 - mse: 0.0462 - val_loss: 1.9945 - val_acc: 0.4108 - val_mse: 0.0893\n",
      "Epoch 113/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.8016 - acc: 0.7242 - mse: 0.0465\n",
      "Epoch 00113: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 995ms/step - loss: 0.8016 - acc: 0.7242 - mse: 0.0465 - val_loss: 2.0907 - val_acc: 0.4534 - val_mse: 0.0885\n",
      "Epoch 114/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7549 - acc: 0.7372 - mse: 0.0442\n",
      "Epoch 00114: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7549 - acc: 0.7372 - mse: 0.0442 - val_loss: 2.1517 - val_acc: 0.4715 - val_mse: 0.0873\n",
      "Epoch 115/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7270 - acc: 0.7493 - mse: 0.0426\n",
      "Epoch 00115: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 0.7270 - acc: 0.7493 - mse: 0.0426 - val_loss: 2.3227 - val_acc: 0.4712 - val_mse: 0.0895\n",
      "Epoch 116/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7337 - acc: 0.7426 - mse: 0.0429\n",
      "Epoch 00116: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7337 - acc: 0.7426 - mse: 0.0429 - val_loss: 2.1123 - val_acc: 0.4613 - val_mse: 0.0877\n",
      "Epoch 117/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7173 - acc: 0.7528 - mse: 0.0417\n",
      "Epoch 00117: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 995ms/step - loss: 0.7173 - acc: 0.7528 - mse: 0.0417 - val_loss: 2.1442 - val_acc: 0.4493 - val_mse: 0.0889\n",
      "Epoch 118/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7318 - acc: 0.7522 - mse: 0.0426\n",
      "Epoch 00118: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7318 - acc: 0.7522 - mse: 0.0426 - val_loss: 2.1359 - val_acc: 0.4565 - val_mse: 0.0886\n",
      "Epoch 119/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7303 - acc: 0.7508 - mse: 0.0424\n",
      "Epoch 00119: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7303 - acc: 0.7508 - mse: 0.0424 - val_loss: 2.2665 - val_acc: 0.4728 - val_mse: 0.0893\n",
      "Epoch 120/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7114 - acc: 0.7631 - mse: 0.0403\n",
      "Epoch 00120: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7114 - acc: 0.7631 - mse: 0.0403 - val_loss: 2.1744 - val_acc: 0.4495 - val_mse: 0.0893\n",
      "Epoch 121/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6912 - acc: 0.7689 - mse: 0.0399\n",
      "Epoch 00121: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6912 - acc: 0.7689 - mse: 0.0399 - val_loss: 2.2385 - val_acc: 0.4736 - val_mse: 0.0879\n",
      "Epoch 122/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6936 - acc: 0.7673 - mse: 0.0400\n",
      "Epoch 00122: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6936 - acc: 0.7673 - mse: 0.0400 - val_loss: 2.2184 - val_acc: 0.4385 - val_mse: 0.0910\n",
      "Epoch 123/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6670 - acc: 0.7715 - mse: 0.0385\n",
      "Epoch 00123: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6670 - acc: 0.7715 - mse: 0.0385 - val_loss: 2.2663 - val_acc: 0.4731 - val_mse: 0.0884\n",
      "Epoch 124/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6715 - acc: 0.7684 - mse: 0.0386\n",
      "Epoch 00124: val_acc did not improve from 0.48767\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6715 - acc: 0.7684 - mse: 0.0386 - val_loss: 2.1250 - val_acc: 0.4692 - val_mse: 0.0880\n",
      "Epoch 125/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6640 - acc: 0.7778 - mse: 0.0380\n",
      "Epoch 00125: val_acc improved from 0.48767 to 0.50314, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 14s 1s/step - loss: 0.6640 - acc: 0.7778 - mse: 0.0380 - val_loss: 2.4950 - val_acc: 0.5031 - val_mse: 0.0890\n",
      "Epoch 126/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7155 - acc: 0.7623 - mse: 0.0410\n",
      "Epoch 00126: val_acc did not improve from 0.50314\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7155 - acc: 0.7623 - mse: 0.0410 - val_loss: 1.9902 - val_acc: 0.4399 - val_mse: 0.0880\n",
      "Epoch 127/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7243 - acc: 0.7600 - mse: 0.0416\n",
      "Epoch 00127: val_acc did not improve from 0.50314\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7243 - acc: 0.7600 - mse: 0.0416 - val_loss: 2.1400 - val_acc: 0.4455 - val_mse: 0.0905\n",
      "Epoch 128/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6972 - acc: 0.7638 - mse: 0.0407\n",
      "Epoch 00128: val_acc did not improve from 0.50314\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6972 - acc: 0.7638 - mse: 0.0407 - val_loss: 2.1080 - val_acc: 0.4420 - val_mse: 0.0913\n",
      "Epoch 129/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6685 - acc: 0.7742 - mse: 0.0391\n",
      "Epoch 00129: val_acc did not improve from 0.50314\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6685 - acc: 0.7742 - mse: 0.0391 - val_loss: 2.2683 - val_acc: 0.4607 - val_mse: 0.0914\n",
      "Epoch 130/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6782 - acc: 0.7750 - mse: 0.0387\n",
      "Epoch 00130: val_acc did not improve from 0.50314\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6782 - acc: 0.7750 - mse: 0.0387 - val_loss: 2.3167 - val_acc: 0.4582 - val_mse: 0.0907\n",
      "Epoch 131/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6536 - acc: 0.7833 - mse: 0.0375\n",
      "Epoch 00131: val_acc did not improve from 0.50314\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6536 - acc: 0.7833 - mse: 0.0375 - val_loss: 2.3009 - val_acc: 0.4569 - val_mse: 0.0917\n",
      "Epoch 132/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6437 - acc: 0.7830 - mse: 0.0372\n",
      "Epoch 00132: val_acc did not improve from 0.50314\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6437 - acc: 0.7830 - mse: 0.0372 - val_loss: 2.1127 - val_acc: 0.4732 - val_mse: 0.0875\n",
      "Epoch 133/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6680 - acc: 0.7793 - mse: 0.0387\n",
      "Epoch 00133: val_acc improved from 0.50314 to 0.50598, saving model to /global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\n",
      "13/13 [==============================] - 14s 1s/step - loss: 0.6680 - acc: 0.7793 - mse: 0.0387 - val_loss: 2.2195 - val_acc: 0.5060 - val_mse: 0.0862\n",
      "Epoch 134/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6861 - acc: 0.7710 - mse: 0.0393\n",
      "Epoch 00134: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6861 - acc: 0.7710 - mse: 0.0393 - val_loss: 2.3053 - val_acc: 0.4424 - val_mse: 0.0934\n",
      "Epoch 135/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6587 - acc: 0.7834 - mse: 0.0379\n",
      "Epoch 00135: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6587 - acc: 0.7834 - mse: 0.0379 - val_loss: 2.2334 - val_acc: 0.4719 - val_mse: 0.0904\n",
      "Epoch 136/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6374 - acc: 0.7895 - mse: 0.0365\n",
      "Epoch 00136: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6374 - acc: 0.7895 - mse: 0.0365 - val_loss: 2.2127 - val_acc: 0.4802 - val_mse: 0.0881\n",
      "Epoch 137/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6152 - acc: 0.8006 - mse: 0.0348\n",
      "Epoch 00137: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6152 - acc: 0.8006 - mse: 0.0348 - val_loss: 2.1127 - val_acc: 0.4443 - val_mse: 0.0921\n",
      "Epoch 138/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6874 - acc: 0.7781 - mse: 0.0393\n",
      "Epoch 00138: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6874 - acc: 0.7781 - mse: 0.0393 - val_loss: 2.0522 - val_acc: 0.4616 - val_mse: 0.0885\n",
      "Epoch 139/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6447 - acc: 0.7902 - mse: 0.0371\n",
      "Epoch 00139: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6447 - acc: 0.7902 - mse: 0.0371 - val_loss: 2.0323 - val_acc: 0.4322 - val_mse: 0.0920\n",
      "Epoch 140/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6260 - acc: 0.7918 - mse: 0.0360\n",
      "Epoch 00140: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1000ms/step - loss: 0.6260 - acc: 0.7918 - mse: 0.0360 - val_loss: 2.5345 - val_acc: 0.4799 - val_mse: 0.0932\n",
      "Epoch 141/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6354 - acc: 0.7833 - mse: 0.0367\n",
      "Epoch 00141: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6354 - acc: 0.7833 - mse: 0.0367 - val_loss: 2.1118 - val_acc: 0.4641 - val_mse: 0.0906\n",
      "Epoch 142/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6433 - acc: 0.7930 - mse: 0.0371\n",
      "Epoch 00142: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.6433 - acc: 0.7930 - mse: 0.0371 - val_loss: 2.0795 - val_acc: 0.4421 - val_mse: 0.0939\n",
      "Epoch 143/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6293 - acc: 0.7924 - mse: 0.0365\n",
      "Epoch 00143: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6293 - acc: 0.7924 - mse: 0.0365 - val_loss: 2.1082 - val_acc: 0.4914 - val_mse: 0.0883\n",
      "Epoch 144/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6468 - acc: 0.7884 - mse: 0.0369\n",
      "Epoch 00144: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.6468 - acc: 0.7884 - mse: 0.0369 - val_loss: 1.9665 - val_acc: 0.4977 - val_mse: 0.0851\n",
      "Epoch 145/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6062 - acc: 0.8002 - mse: 0.0344\n",
      "Epoch 00145: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6062 - acc: 0.8002 - mse: 0.0344 - val_loss: 2.1109 - val_acc: 0.4480 - val_mse: 0.0925\n",
      "Epoch 146/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6247 - acc: 0.7965 - mse: 0.0352\n",
      "Epoch 00146: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.6247 - acc: 0.7965 - mse: 0.0352 - val_loss: 2.0669 - val_acc: 0.4404 - val_mse: 0.0927\n",
      "Epoch 147/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6129 - acc: 0.8048 - mse: 0.0346\n",
      "Epoch 00147: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1000ms/step - loss: 0.6129 - acc: 0.8048 - mse: 0.0346 - val_loss: 2.1140 - val_acc: 0.4789 - val_mse: 0.0896\n",
      "Epoch 148/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5852 - acc: 0.8175 - mse: 0.0328\n",
      "Epoch 00148: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5852 - acc: 0.8175 - mse: 0.0328 - val_loss: 2.3343 - val_acc: 0.4861 - val_mse: 0.0908\n",
      "Epoch 149/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5712 - acc: 0.8152 - mse: 0.0326\n",
      "Epoch 00149: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5712 - acc: 0.8152 - mse: 0.0326 - val_loss: 2.2944 - val_acc: 0.4794 - val_mse: 0.0915\n",
      "Epoch 150/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5724 - acc: 0.8134 - mse: 0.0325\n",
      "Epoch 00150: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5724 - acc: 0.8134 - mse: 0.0325 - val_loss: 2.1676 - val_acc: 0.4429 - val_mse: 0.0939\n",
      "Epoch 151/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5861 - acc: 0.8135 - mse: 0.0332\n",
      "Epoch 00151: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5861 - acc: 0.8135 - mse: 0.0332 - val_loss: 2.2317 - val_acc: 0.4901 - val_mse: 0.0898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5597 - acc: 0.8194 - mse: 0.0315\n",
      "Epoch 00152: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1000ms/step - loss: 0.5597 - acc: 0.8194 - mse: 0.0315 - val_loss: 2.3104 - val_acc: 0.4667 - val_mse: 0.0923\n",
      "Epoch 153/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5355 - acc: 0.8239 - mse: 0.0306\n",
      "Epoch 00153: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5355 - acc: 0.8239 - mse: 0.0306 - val_loss: 2.3733 - val_acc: 0.4578 - val_mse: 0.0930\n",
      "Epoch 154/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5418 - acc: 0.8238 - mse: 0.0307\n",
      "Epoch 00154: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5418 - acc: 0.8238 - mse: 0.0307 - val_loss: 2.2204 - val_acc: 0.4811 - val_mse: 0.0897\n",
      "Epoch 155/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5380 - acc: 0.8268 - mse: 0.0306\n",
      "Epoch 00155: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5380 - acc: 0.8268 - mse: 0.0306 - val_loss: 2.1693 - val_acc: 0.4785 - val_mse: 0.0892\n",
      "Epoch 156/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5280 - acc: 0.8298 - mse: 0.0299\n",
      "Epoch 00156: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5280 - acc: 0.8298 - mse: 0.0299 - val_loss: 2.1985 - val_acc: 0.4523 - val_mse: 0.0917\n",
      "Epoch 157/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5303 - acc: 0.8231 - mse: 0.0306\n",
      "Epoch 00157: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5303 - acc: 0.8231 - mse: 0.0306 - val_loss: 2.2268 - val_acc: 0.4612 - val_mse: 0.0913\n",
      "Epoch 158/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5281 - acc: 0.8285 - mse: 0.0300\n",
      "Epoch 00158: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5281 - acc: 0.8285 - mse: 0.0300 - val_loss: 2.1821 - val_acc: 0.4666 - val_mse: 0.0915\n",
      "Epoch 159/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5370 - acc: 0.8244 - mse: 0.0305\n",
      "Epoch 00159: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5370 - acc: 0.8244 - mse: 0.0305 - val_loss: 2.2016 - val_acc: 0.4881 - val_mse: 0.0882\n",
      "Epoch 160/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5295 - acc: 0.8269 - mse: 0.0303\n",
      "Epoch 00160: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5295 - acc: 0.8269 - mse: 0.0303 - val_loss: 2.2024 - val_acc: 0.4759 - val_mse: 0.0898\n",
      "Epoch 161/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5369 - acc: 0.8217 - mse: 0.0308\n",
      "Epoch 00161: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5369 - acc: 0.8217 - mse: 0.0308 - val_loss: 2.2510 - val_acc: 0.4782 - val_mse: 0.0885\n",
      "Epoch 162/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5427 - acc: 0.8303 - mse: 0.0304\n",
      "Epoch 00162: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5427 - acc: 0.8303 - mse: 0.0304 - val_loss: 2.4437 - val_acc: 0.4750 - val_mse: 0.0923\n",
      "Epoch 163/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5837 - acc: 0.8165 - mse: 0.0323\n",
      "Epoch 00163: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5837 - acc: 0.8165 - mse: 0.0323 - val_loss: 2.5374 - val_acc: 0.4771 - val_mse: 0.0963\n",
      "Epoch 164/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5853 - acc: 0.8137 - mse: 0.0325\n",
      "Epoch 00164: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5853 - acc: 0.8137 - mse: 0.0325 - val_loss: 2.4991 - val_acc: 0.4737 - val_mse: 0.0954\n",
      "Epoch 165/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5922 - acc: 0.8141 - mse: 0.0328\n",
      "Epoch 00165: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5922 - acc: 0.8141 - mse: 0.0328 - val_loss: 2.0872 - val_acc: 0.4127 - val_mse: 0.0971\n",
      "Epoch 166/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5822 - acc: 0.8110 - mse: 0.0322\n",
      "Epoch 00166: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5822 - acc: 0.8110 - mse: 0.0322 - val_loss: 1.9933 - val_acc: 0.4543 - val_mse: 0.0910\n",
      "Epoch 167/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5680 - acc: 0.8251 - mse: 0.0318\n",
      "Epoch 00167: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5680 - acc: 0.8251 - mse: 0.0318 - val_loss: 2.2273 - val_acc: 0.4754 - val_mse: 0.0908\n",
      "Epoch 168/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5315 - acc: 0.8294 - mse: 0.0298\n",
      "Epoch 00168: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5315 - acc: 0.8294 - mse: 0.0298 - val_loss: 2.8090 - val_acc: 0.4675 - val_mse: 0.1002\n",
      "Epoch 169/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6009 - acc: 0.8129 - mse: 0.0335\n",
      "Epoch 00169: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6009 - acc: 0.8129 - mse: 0.0335 - val_loss: 2.4394 - val_acc: 0.4974 - val_mse: 0.0926\n",
      "Epoch 170/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6057 - acc: 0.8057 - mse: 0.0342\n",
      "Epoch 00170: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6057 - acc: 0.8057 - mse: 0.0342 - val_loss: 2.3738 - val_acc: 0.4630 - val_mse: 0.0952\n",
      "Epoch 171/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5760 - acc: 0.8150 - mse: 0.0325\n",
      "Epoch 00171: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 0.5760 - acc: 0.8150 - mse: 0.0325 - val_loss: 2.3517 - val_acc: 0.4604 - val_mse: 0.0973\n",
      "Epoch 172/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5926 - acc: 0.8082 - mse: 0.0339\n",
      "Epoch 00172: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.5926 - acc: 0.8082 - mse: 0.0339 - val_loss: 2.3709 - val_acc: 0.4951 - val_mse: 0.0913\n",
      "Epoch 173/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3190 - acc: 0.6122 - mse: 0.0661\n",
      "Epoch 00173: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 1.3190 - acc: 0.6122 - mse: 0.0661 - val_loss: 1.7691 - val_acc: 0.4652 - val_mse: 0.0936\n",
      "Epoch 174/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.8212 - acc: 0.3722 - mse: 0.0979\n",
      "Epoch 00174: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 991ms/step - loss: 1.8212 - acc: 0.3722 - mse: 0.0979 - val_loss: 1.7195 - val_acc: 0.4679 - val_mse: 0.0919\n",
      "Epoch 175/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.6170 - acc: 0.4715 - mse: 0.0880\n",
      "Epoch 00175: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 997ms/step - loss: 1.6170 - acc: 0.4715 - mse: 0.0880 - val_loss: 1.7891 - val_acc: 0.4692 - val_mse: 0.0902\n",
      "Epoch 176/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.5084 - acc: 0.4990 - mse: 0.0815\n",
      "Epoch 00176: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 1.5084 - acc: 0.4990 - mse: 0.0815 - val_loss: 1.7330 - val_acc: 0.4819 - val_mse: 0.0881\n",
      "Epoch 177/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3615 - acc: 0.5320 - mse: 0.0744\n",
      "Epoch 00177: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 999ms/step - loss: 1.3615 - acc: 0.5320 - mse: 0.0744 - val_loss: 1.7123 - val_acc: 0.5037 - val_mse: 0.0860\n",
      "Epoch 178/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.2445 - acc: 0.5869 - mse: 0.0689\n",
      "Epoch 00178: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 1.2445 - acc: 0.5869 - mse: 0.0689 - val_loss: 1.8159 - val_acc: 0.4793 - val_mse: 0.0874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.1094 - acc: 0.6273 - mse: 0.0619\n",
      "Epoch 00179: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 998ms/step - loss: 1.1094 - acc: 0.6273 - mse: 0.0619 - val_loss: 1.9396 - val_acc: 0.5028 - val_mse: 0.0892\n",
      "Epoch 180/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9935 - acc: 0.6721 - mse: 0.0559\n",
      "Epoch 00180: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.9935 - acc: 0.6721 - mse: 0.0559 - val_loss: 1.7898 - val_acc: 0.4636 - val_mse: 0.0877\n",
      "Epoch 181/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.9222 - acc: 0.7105 - mse: 0.0512\n",
      "Epoch 00181: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.9222 - acc: 0.7105 - mse: 0.0512 - val_loss: 1.9012 - val_acc: 0.4618 - val_mse: 0.0900\n",
      "Epoch 182/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.7708 - acc: 0.7525 - mse: 0.0436\n",
      "Epoch 00182: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.7708 - acc: 0.7525 - mse: 0.0436 - val_loss: 2.1420 - val_acc: 0.4823 - val_mse: 0.0908\n",
      "Epoch 183/1000\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6788 - acc: 0.7839 - mse: 0.0386\n",
      "Epoch 00183: val_acc did not improve from 0.50598\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6788 - acc: 0.7839 - mse: 0.0386 - val_loss: 2.1189 - val_acc: 0.4715 - val_mse: 0.0908\n",
      "Epoch 00183: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Modelo\n",
    "loss=\"categorical_crossentropy\"\n",
    "opt = Adam(lr = 0.0001,epsilon=1e-08)\n",
    "\n",
    "model = GoogleNet()\n",
    "\n",
    "model.compile(loss=loss, optimizer=opt, metrics=['acc', 'mse'])      \n",
    "model.summary() #ver resumen red\n",
    "\n",
    "# regularizadores\n",
    "log_dir = main_path+\"models/v4/googleNet-v4/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tb = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "mc = ModelCheckpoint(main_path+'models/v4/googleNet-v4/models/model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_acc',mode='max',patience=50, verbose=1)\n",
    "\n",
    "# fit model\n",
    "#posiblemente cambiar cant de epochs\n",
    "gglNet_HISTORY = model.fit(x=x_train, y=y_train, batch_size=128, epochs=1000, validation_data=(x_val,y_val), shuffle=True, verbose = 1,callbacks=[es,mc,tb])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      " 1/34 [..............................] - ETA: 0sWARNING:tensorflow:Callbacks method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_predict_batch_end` time: 0.0732s). Check your callbacks.\n",
      "34/34 [==============================] - 4s 111ms/step\n"
     ]
    }
   ],
   "source": [
    "#Save info\n",
    "model_json = model.to_json()\n",
    "with open(main_path+\"models/v4/googleNet-v4/models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(main_path+\"models/v4/googleNet-v4/models/model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "f = open(main_path+\"models/v4/googleNet-v4/results/y_test.txt\", \"w\")\n",
    "np.savetxt(f,y_test.reshape(y_test.shape[0], -1))\n",
    "f.close()\n",
    "\n",
    "f = open(main_path+\"models/v4/googleNet-v4/results/y_train.txt\", \"w\")\n",
    "np.savetxt(f,y_train.reshape(y_train.shape[0], -1))\n",
    "f.close()\n",
    "\n",
    "y_pred_onehot = model.predict(x_test, verbose=1)\n",
    "f = open(main_path+\"models/v4/googleNet-v4/results/y_preds_onehot.txt\", \"w\")\n",
    "np.savetxt(f,y_pred_onehot.reshape(y_pred_onehot.shape[0], -1))\n",
    "f.close()\n",
    "\n",
    "y_pred = np.argmax(y_pred_onehot, axis=2)\n",
    "f = open(main_path+\"models/v4/googleNet-v4/results/y_preds.txt\", \"w\")\n",
    "np.savetxt(f,y_pred.reshape(y_pred.shape[0], -1))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 3s 93ms/step - loss: 2.0466 - acc: 0.4932 - mse: 0.0884\n",
      "MODEL Metric names:  ['loss', 'acc', 'mse']\n",
      "loss: 2.046595573425293\n",
      "accuracy: 0.493193656206131\n",
      "mse: 0.08839840441942215\n",
      "HISTORY Keys:  dict_keys(['loss', 'acc', 'mse', 'val_loss', 'val_acc', 'val_mse'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Graficos\n",
    "'''\n",
    "#Matriz de confusion\n",
    "lab = [x for x in range(0,num_class)]\n",
    "matrix = multilabel_confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1), labels = lab)\n",
    "print(matrix)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(matrix, annot=True, ax = ax); #annot=True to annotate cells\n",
    "plt.savefig(fname = \"/global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/results/confMat.png\")\n",
    "'''\n",
    "#model.load_weights(\"/global/scratch/users/cpezov/AAPBO/models/v4/googleNet-v4/models/model.h5\") #cargalo de tu ruta\n",
    "score = model.evaluate(x_test, y_test,verbose=1)\n",
    "print(\"MODEL Metric names: \", model.metrics_names)\n",
    "print(\"loss:\", score[0])\n",
    "print(\"accuracy:\", score[1])\n",
    "print(\"mse:\", score[2])\n",
    "\n",
    "print(\"HISTORY Keys: \", gglNet_HISTORY.history.keys())\n",
    "plt.figure(0)\n",
    "plt.plot(gglNet_HISTORY.history['acc'])\n",
    "plt.plot(gglNet_HISTORY.history['val_acc'])\n",
    "plt.title(\"Training Accuracy vs Validation Accuracy\") \n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.savefig(main_path+'models/v4/googleNet-v4/results/acc_graph.png')\n",
    "\n",
    "# summarize history for loss\n",
    "plt.figure(1)\n",
    "plt.plot(gglNet_HISTORY.history['loss'])\n",
    "plt.plot(gglNet_HISTORY.history['val_loss'])\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "plt.savefig(main_path+'models/v4/googleNet-v4/results/loss_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Model: \"GoogleNet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 125, 125, 64) 3200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 62, 62, 64)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 62, 62, 64)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 62, 62, 192)  110784      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 30, 30, 192)  0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 30, 30, 96)   18528       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 30, 30, 16)   3088        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 30, 30, 192)  0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 30, 30, 64)   12352       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 30, 30, 128)  110720      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 30, 30, 32)   12832       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 30, 30, 32)   6176        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 30, 30, 256)  0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 30, 30, 128)  32896       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 30, 30, 32)   8224        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 30, 30, 256)  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 30, 30, 128)  32896       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 30, 30, 192)  221376      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 30, 30, 96)   76896       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 30, 30, 64)   16448       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 30, 480)  0           conv2d_9[0][0]                   \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 480)  0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 14, 14, 96)   46176       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 14, 14, 16)   7696        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 14, 14, 480)  0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 14, 14, 192)  92352       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 14, 14, 208)  179920      conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 14, 14, 48)   19248       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 14, 14, 64)   30784       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 14, 14, 512)  0           conv2d_15[0][0]                  \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 14, 14, 112)  57456       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 14, 14, 24)   12312       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 14, 14, 512)  0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 14, 14, 160)  82080       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 14, 14, 224)  226016      conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 14, 14, 64)   38464       conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 14, 14, 64)   32832       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 14, 14, 512)  0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 14, 14, 128)  65664       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 14, 14, 24)   12312       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 14, 14, 512)  0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 14, 14, 128)  65664       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 14, 14, 256)  295168      conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 14, 14, 64)   38464       conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 14, 14, 64)   32832       max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 14, 14, 512)  0           conv2d_28[0][0]                  \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 14, 14, 144)  73872       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 14, 14, 32)   16416       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 14, 14, 512)  0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 14, 14, 112)  57456       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 14, 14, 288)  373536      conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 14, 14, 64)   51264       conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 14, 14, 64)   32832       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 14, 14, 528)  0           conv2d_34[0][0]                  \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 14, 14, 160)  84640       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 14, 14, 32)   16928       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 14, 14, 528)  0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 14, 14, 256)  135424      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 14, 14, 320)  461120      conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 14, 14, 128)  102528      conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 14, 14, 128)  67712       max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 14, 14, 832)  0           conv2d_41[0][0]                  \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "                                                                 conv2d_45[0][0]                  \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 6, 6, 832)    0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 6, 6, 160)    133280      max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 6, 6, 32)     26656       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 6, 6, 832)    0           max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 6, 6, 256)    213248      max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 6, 6, 320)    461120      conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 6, 6, 128)    102528      conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 6, 6, 128)    106624      max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 6, 6, 832)    0           conv2d_47[0][0]                  \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "                                                                 conv2d_51[0][0]                  \n",
      "                                                                 conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 6, 6, 192)    159936      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 6, 6, 48)     39984       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 6, 6, 832)    0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 6, 6, 384)    319872      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 6, 6, 384)    663936      conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 6, 6, 128)    153728      conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 6, 6, 128)    106624      max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 6, 6, 1024)   0           conv2d_53[0][0]                  \n",
      "                                                                 conv2d_55[0][0]                  \n",
      "                                                                 conv2d_57[0][0]                  \n",
      "                                                                 conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "GAPL (GlobalAveragePooling2D)   (None, 1024)         0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           65600       GAPL[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4000)         260000      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 500, 8)       0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Activation)             (None, 500, 8)       0           reshape[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,292,880\n",
      "Trainable params: 6,292,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "\n",
    "# load json and create model\n",
    "json_file = open(main_path+\"models/v4/googleNet-v4/models/model.json\", 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "# load weights into new model\n",
    "model.load_weights(main_path+\"models/v4/googleNet-v4/models/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loss=\"categorical_crossentropy\"\n",
    "opt = Adam(lr = 0.0001,epsilon=1e-08)\n",
    "\n",
    "model.compile(loss=loss, optimizer=opt, metrics=['acc', 'mse'])      \n",
    "model.summary() #ver resumen red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 1s 22ms/step\n",
      "34/34 [==============================] - 1s 17ms/step - loss: 2.0466 - acc: 0.4932 - mse: 0.0884\n",
      "MODEL Metric names:  ['loss', 'acc', 'mse']\n",
      "loss: 2.0466086864471436\n",
      "accuracy: 0.4931899309158325\n",
      "mse: 0.08839842677116394\n"
     ]
    }
   ],
   "source": [
    "#Verify model has same metrics\n",
    "\n",
    "y_pred_onehot = model.predict(x_test, verbose=1)\n",
    "y_pred = np.argmax(y_pred_onehot, axis=2)\n",
    "\n",
    "score = model.evaluate(x_test, y_test,verbose=1)\n",
    "print(\"MODEL Metric names: \", model.metrics_names)\n",
    "print(\"loss:\", score[0])\n",
    "print(\"accuracy:\", score[1])\n",
    "print(\"mse:\", score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-05 17:19:28.591690\n"
     ]
    }
   ],
   "source": [
    "#Calcular tiempos de predicción por cada par instancia-tiempo de test\n",
    "#(Correr esto se demora 1 hora aprox)\n",
    "\n",
    "now=datetime.datetime.now()\n",
    "print(now)\n",
    "\n",
    "l=int(len(y_pred))\n",
    "t_preds=np.empty(l*500) #Una predicción da los resultados de todos los 500 timesteps\n",
    "\n",
    "#prediccion\n",
    "for i in range(len(x_test)):\n",
    "    #crear np.array de solo esa instancia-timestep\n",
    "    x_i = [x_test[i]]\n",
    "    x_i = np.array(x_i)\n",
    "    \n",
    "    #hacer predicción y medir tiempo\n",
    "    start=time.time()\n",
    "    y_i = model.predict(x_i)\n",
    "    tiempo=time.time()-start\n",
    "    #Por cada instancia, guardar el mismo tiempo de predicción para los 500 timesteps (para obtener el resultado de 1 timestep, el modelo debe obtener la predicción para todos los timesteps, por lo que el tiempo que toma es el mismo para todos los timesteps)\n",
    "    for j in range(500): \n",
    "        t_preds[500*i+j]=tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24367619 0.24367619 0.24367619 ... 0.07071996 0.07071996 0.07071996]\n"
     ]
    }
   ],
   "source": [
    "#Guardar tiempos de prediccion\n",
    "\n",
    "print(t_preds)\n",
    "f = open(main_path+\"models/results-v4/pred_times/t_pred_googleNet-v4.txt\", \"w\")\n",
    "np.savetxt(f, t_preds, delimiter=\" \", fmt=\"%s\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-05 17:20:47.445289\n"
     ]
    }
   ],
   "source": [
    "now=datetime.datetime.now()\n",
    "print(now)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cnn_GoogleNet",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3.7 TF-2.3.0",
   "language": "python",
   "name": "python3.7-tf2.3.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
